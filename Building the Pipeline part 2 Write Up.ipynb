{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import pipeline.util as u\n",
    "import pipeline.process as pr\n",
    "import pipeline.read as r\n",
    "import pipeline.explore as ex\n",
    "import pipeline.evaluate as ev\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I was working on improving my previous homework and determining a method for feature selection, I ran the small grid search of Magic Loops with the specifications Rayid selected. This ran over night, but unfortunately I ran it before I thought to add a timer. From observation the gradient boosting and ADA boosting appeared to be some of the slower models. The results informed the smaller loop I ran on the features selected by Random Forest, which were nearly identical to Rayid's feature set except I included monthly_income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>clf</th>\n",
       "      <th>parameters</th>\n",
       "      <th>auc-roc</th>\n",
       "      <th>p_at_5</th>\n",
       "      <th>p_at_10</th>\n",
       "      <th>p_at_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.5, 'max_depth': 50, 'n_est...</td>\n",
       "      <td>0.480473</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.107333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.5, 'max_depth': 50, 'n_est...</td>\n",
       "      <td>0.484790</td>\n",
       "      <td>0.220267</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='kd_tree', leaf...</td>\n",
       "      <td>{'algorithm': 'ball_tree', 'n_neighbors': 1, '...</td>\n",
       "      <td>0.567491</td>\n",
       "      <td>0.258133</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='kd_tree', leaf...</td>\n",
       "      <td>{'algorithm': 'ball_tree', 'n_neighbors': 1, '...</td>\n",
       "      <td>0.567491</td>\n",
       "      <td>0.258133</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='kd_tree', leaf...</td>\n",
       "      <td>{'algorithm': 'kd_tree', 'n_neighbors': 1, 'we...</td>\n",
       "      <td>0.567687</td>\n",
       "      <td>0.258667</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='kd_tree', leaf...</td>\n",
       "      <td>{'algorithm': 'auto', 'n_neighbors': 1, 'weigh...</td>\n",
       "      <td>0.567687</td>\n",
       "      <td>0.258667</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='kd_tree', leaf...</td>\n",
       "      <td>{'algorithm': 'kd_tree', 'n_neighbors': 1, 'we...</td>\n",
       "      <td>0.567687</td>\n",
       "      <td>0.258667</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>KNN</td>\n",
       "      <td>KNeighborsClassifier(algorithm='kd_tree', leaf...</td>\n",
       "      <td>{'algorithm': 'auto', 'n_neighbors': 1, 'weigh...</td>\n",
       "      <td>0.567687</td>\n",
       "      <td>0.258667</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 50, 'n_est...</td>\n",
       "      <td>0.568171</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.194667</td>\n",
       "      <td>0.145067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.5, 'max_depth': 50, 'n_est...</td>\n",
       "      <td>0.568612</td>\n",
       "      <td>0.289067</td>\n",
       "      <td>0.202933</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_type                                                clf  \\\n",
       "199         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "196         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "101        KNN  KNeighborsClassifier(algorithm='kd_tree', leaf...   \n",
       "100        KNN  KNeighborsClassifier(algorithm='kd_tree', leaf...   \n",
       "113        KNN  KNeighborsClassifier(algorithm='kd_tree', leaf...   \n",
       "88         KNN  KNeighborsClassifier(algorithm='kd_tree', leaf...   \n",
       "112        KNN  KNeighborsClassifier(algorithm='kd_tree', leaf...   \n",
       "89         KNN  KNeighborsClassifier(algorithm='kd_tree', leaf...   \n",
       "187         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "198         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "\n",
       "                                            parameters   auc-roc    p_at_5  \\\n",
       "199  {'learning_rate': 0.5, 'max_depth': 50, 'n_est...  0.480473  0.272000   \n",
       "196  {'learning_rate': 0.5, 'max_depth': 50, 'n_est...  0.484790  0.220267   \n",
       "101  {'algorithm': 'ball_tree', 'n_neighbors': 1, '...  0.567491  0.258133   \n",
       "100  {'algorithm': 'ball_tree', 'n_neighbors': 1, '...  0.567491  0.258133   \n",
       "113  {'algorithm': 'kd_tree', 'n_neighbors': 1, 'we...  0.567687  0.258667   \n",
       "88   {'algorithm': 'auto', 'n_neighbors': 1, 'weigh...  0.567687  0.258667   \n",
       "112  {'algorithm': 'kd_tree', 'n_neighbors': 1, 'we...  0.567687  0.258667   \n",
       "89   {'algorithm': 'auto', 'n_neighbors': 1, 'weigh...  0.567687  0.258667   \n",
       "187  {'learning_rate': 0.1, 'max_depth': 50, 'n_est...  0.568171  0.188800   \n",
       "198  {'learning_rate': 0.5, 'max_depth': 50, 'n_est...  0.568612  0.289067   \n",
       "\n",
       "      p_at_10   p_at_20  \n",
       "199  0.136000  0.107333  \n",
       "196  0.149600  0.112000  \n",
       "101  0.486933  0.339600  \n",
       "100  0.486933  0.339600  \n",
       "113  0.486933  0.339600  \n",
       "88   0.486933  0.339600  \n",
       "112  0.486933  0.339600  \n",
       "89   0.486933  0.339600  \n",
       "187  0.194667  0.145067  \n",
       "198  0.202933  0.148000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_grid = r.read_csv(\"small_loop_result.csv\")\n",
    "fg = first_grid.sort_values(by=\"auc-roc\")\n",
    "fg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>clf</th>\n",
       "      <th>parameters</th>\n",
       "      <th>auc-roc</th>\n",
       "      <th>p_at_5</th>\n",
       "      <th>p_at_10</th>\n",
       "      <th>p_at_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>0.830078</td>\n",
       "      <td>0.443733</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.223733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>0.829089</td>\n",
       "      <td>0.448533</td>\n",
       "      <td>0.321867</td>\n",
       "      <td>0.224933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.5, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>0.828998</td>\n",
       "      <td>0.442667</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>AB</td>\n",
       "      <td>AdaBoostClassifier(algorithm='SAMME.R',\\r     ...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'n_estimators': 100}</td>\n",
       "      <td>0.827574</td>\n",
       "      <td>0.437333</td>\n",
       "      <td>0.319467</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.826962</td>\n",
       "      <td>0.449067</td>\n",
       "      <td>0.322133</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>0.826932</td>\n",
       "      <td>0.450133</td>\n",
       "      <td>0.322667</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>0.826761</td>\n",
       "      <td>0.451733</td>\n",
       "      <td>0.322933</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.826650</td>\n",
       "      <td>0.443733</td>\n",
       "      <td>0.321067</td>\n",
       "      <td>0.223467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.826459</td>\n",
       "      <td>0.448533</td>\n",
       "      <td>0.322133</td>\n",
       "      <td>0.222133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>0.826454</td>\n",
       "      <td>0.451733</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>0.222800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_type                                                clf  \\\n",
       "183         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "182         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "192         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "163         AB  AdaBoostClassifier(algorithm='SAMME.R',\\r     ...   \n",
       "3           RF  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "5           RF  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "7           RF  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "0           RF  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "1           RF  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "179         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "\n",
       "                                            parameters   auc-roc    p_at_5  \\\n",
       "183  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.830078  0.443733   \n",
       "182  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.829089  0.448533   \n",
       "192  {'learning_rate': 0.5, 'max_depth': 5, 'n_esti...  0.828998  0.442667   \n",
       "163      {'algorithm': 'SAMME.R', 'n_estimators': 100}  0.827574  0.437333   \n",
       "3    {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.826962  0.449067   \n",
       "5    {'max_depth': 5, 'max_features': 'log2', 'min_...  0.826932  0.450133   \n",
       "7    {'max_depth': 5, 'max_features': 'log2', 'min_...  0.826761  0.451733   \n",
       "0    {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.826650  0.443733   \n",
       "1    {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.826459  0.448533   \n",
       "179  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.826454  0.451733   \n",
       "\n",
       "      p_at_10   p_at_20  \n",
       "183  0.324000  0.223733  \n",
       "182  0.321867  0.224933  \n",
       "192  0.320000  0.225200  \n",
       "163  0.319467  0.222400  \n",
       "3    0.322133  0.222000  \n",
       "5    0.322667  0.222400  \n",
       "7    0.322933  0.222000  \n",
       "0    0.321067  0.223467  \n",
       "1    0.322133  0.222133  \n",
       "179  0.333600  0.222800  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10\n",
    "fg.tail(10).sort_values(by=\"auc-roc\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods performed the best overall, but there was wide variance. Hypertuning made a big difference in model fit according to AUC-ROC. Of the 213 specifications that worked, Gradient boosting classifer resulted in four of the top 10 and four of the bottom ten models. Gradient Boosting classifiers split on max depth, the top performers used max dapth of 5 while the bottom used max depth of 50.\n",
    "\n",
    "Random forests performed very strongly and had much lower variance in the results; AUC-ROC ranged from 72 to 83 percent. (Compare with GB with a range from roughly 48 to 83). Random Forests also split on max depth with shorter trees out performing longer ones. \n",
    "\n",
    "Simpler methods such as Decision Trees and KNN almost matched the ensemble methods under certain specifications. DT specifications with trees of depth 1 captured 65 to 72 percent of the area under the curve, which gives us a baseline of comparison. These models also beat trees with max depth of 20, 50 and 100, which suggest overall that with a limited number of features depths 4 or 5 times greater than the feature set length will overfit. Linear regression performed below the 1-deep trees; AUC-ROC range from 63 to 65 over 10 specifications. Naive bayes hit just above that range. SVD and SGD did not run directly from Rayid's set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>clf</th>\n",
       "      <th>parameters</th>\n",
       "      <th>auc-roc</th>\n",
       "      <th>p_at_5</th>\n",
       "      <th>p_at_10</th>\n",
       "      <th>p_at_20</th>\n",
       "      <th>cnf</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>0.882031</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>0.386133</td>\n",
       "      <td>0.252933</td>\n",
       "      <td>[[34732   221]\\n [ 1813   734]]</td>\n",
       "      <td>15.199776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>0.882008</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>0.385867</td>\n",
       "      <td>0.252933</td>\n",
       "      <td>[[34732   221]\\n [ 1813   734]]</td>\n",
       "      <td>14.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>0.873045</td>\n",
       "      <td>0.566933</td>\n",
       "      <td>0.425600</td>\n",
       "      <td>0.249333</td>\n",
       "      <td>[[34937    16]\\n [ 2212   335]]</td>\n",
       "      <td>1.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>GB</td>\n",
       "      <td>GradientBoostingClassifier(criterion='friedman...</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_esti...</td>\n",
       "      <td>0.873045</td>\n",
       "      <td>0.566933</td>\n",
       "      <td>0.425600</td>\n",
       "      <td>0.249333</td>\n",
       "      <td>[[34937    16]\\n [ 2212   335]]</td>\n",
       "      <td>1.644140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>0.381067</td>\n",
       "      <td>0.246800</td>\n",
       "      <td>[[34844   109]\\n [ 2063   484]]</td>\n",
       "      <td>5.075012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.868082</td>\n",
       "      <td>0.548800</td>\n",
       "      <td>0.373067</td>\n",
       "      <td>0.245867</td>\n",
       "      <td>[[34802   151]\\n [ 2045   502]]</td>\n",
       "      <td>0.644746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'max_fe...</td>\n",
       "      <td>0.865309</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.244933</td>\n",
       "      <td>[[34705   248]\\n [ 1841   706]]</td>\n",
       "      <td>0.181338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'max_fe...</td>\n",
       "      <td>0.862431</td>\n",
       "      <td>0.571733</td>\n",
       "      <td>0.409867</td>\n",
       "      <td>0.245467</td>\n",
       "      <td>[[34693   260]\\n [ 1827   720]]</td>\n",
       "      <td>0.195340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'max_fe...</td>\n",
       "      <td>0.861964</td>\n",
       "      <td>0.557333</td>\n",
       "      <td>0.374933</td>\n",
       "      <td>0.247467</td>\n",
       "      <td>[[34707   246]\\n [ 1858   689]]</td>\n",
       "      <td>0.186253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'max...</td>\n",
       "      <td>0.860982</td>\n",
       "      <td>0.566933</td>\n",
       "      <td>0.421333</td>\n",
       "      <td>0.249867</td>\n",
       "      <td>[[34645   308]\\n [ 1812   735]]</td>\n",
       "      <td>0.422406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_type                                                clf  \\\n",
       "77         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "83         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "82         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "76         GB  GradientBoostingClassifier(criterion='friedman...   \n",
       "1          RF  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "0          RF  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "18         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "17         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "19         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "57         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "\n",
       "                                           parameters   auc-roc    p_at_5  \\\n",
       "77  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.882031  0.574400   \n",
       "83  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.882008  0.574400   \n",
       "82  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.873045  0.566933   \n",
       "76  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.873045  0.566933   \n",
       "1   {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.870229  0.569600   \n",
       "0   {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.868082  0.548800   \n",
       "18  {'criterion': 'gini', 'max_depth': 10, 'max_fe...  0.865309  0.568533   \n",
       "17  {'criterion': 'gini', 'max_depth': 10, 'max_fe...  0.862431  0.571733   \n",
       "19  {'criterion': 'gini', 'max_depth': 10, 'max_fe...  0.861964  0.557333   \n",
       "57  {'criterion': 'entropy', 'max_depth': 10, 'max...  0.860982  0.566933   \n",
       "\n",
       "     p_at_10   p_at_20                              cnf    runtime  \n",
       "77  0.386133  0.252933  [[34732   221]\\n [ 1813   734]]  15.199776  \n",
       "83  0.385867  0.252933  [[34732   221]\\n [ 1813   734]]  14.887500  \n",
       "82  0.425600  0.249333  [[34937    16]\\n [ 2212   335]]   1.665100  \n",
       "76  0.425600  0.249333  [[34937    16]\\n [ 2212   335]]   1.644140  \n",
       "1   0.381067  0.246800  [[34844   109]\\n [ 2063   484]]   5.075012  \n",
       "0   0.373067  0.245867  [[34802   151]\\n [ 2045   502]]   0.644746  \n",
       "18  0.413333  0.244933  [[34705   248]\\n [ 1841   706]]   0.181338  \n",
       "17  0.409867  0.245467  [[34693   260]\\n [ 1827   720]]   0.195340  \n",
       "19  0.374933  0.247467  [[34707   246]\\n [ 1858   689]]   0.186253  \n",
       "57  0.421333  0.249867  [[34645   308]\\n [ 1812   735]]   0.422406  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_grid = r.read_csv(\"refined_results.csv\")\n",
    "sg = second_grid.sort_values(by=\"auc-roc\", ascending=False)\n",
    "sg.head(10)\n",
    "#Discuss speed and how they do with an extra feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding monthly_income increased the AUC-ROC scores for the top performing models by 6 or so points. The top Gradient Boosting algorithm had AUC-ROC of .88, the top RF .87 and the top DT .86. Linear regression and naive bayes did about the same as the first pass, hovering in the .66 range. Adding information did not improve their results much.\n",
    "\n",
    "Time is a major factor in grid search and we see the top GB models takes significantly longer than other models. Dividing the number of estimators by 10 leads to a similar speed gain though. So it seem the algorithm is linear in n_estimators, but gains less than 1% in AUC-ROC. Considering this gain in relation to our baseline of about 72% that performance gain is bigger than it looks. N_estimators had a similar impact on RF though the performance gain was smaller.\n",
    "\n",
    "I included an RF with max_depth 50 in this batch as well. It seems to double the length of time a similarly specified 5-deep RF takes, and produce lower AUC-ROC. However, if we change our focus to finding delinquents, it performed the best finding 768 of 2547 (~30 percent). That seems low, looking at the confusion matrices in the Jupyter notebook, there are high levels of false negatives. This makes sense given that there are a lot more non-delinquents. The best model in this regard found 735 of 2547 delinquents (~29 percent).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
